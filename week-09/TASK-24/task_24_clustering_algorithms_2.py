# -*- coding: utf-8 -*-
"""Task_24 Clustering Algorithms-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ICnul9foFXjZGPJJC1RsSyUOaAMM_fwb

##**Comprehensive Clustering Project – Customer Segmentation Using Mall Customers Dataset**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
sns.set_style("whitegrid")

# Load, clean, and select features
df = pd.read_csv("/content/Mall_Customers.csv")
df.rename(columns={'Annual Income (k$)': 'Income', 'Spending Score (1-100)': 'Spending'}, inplace=True)
X = df[['Income', 'Spending']].values
K = 5 # Optimal cluster count determined by Elbow/Silhouette methods

"""# **EDA**-*Income vs.Spending Scatter Plot*"""

# --- 1. EDA ---
print("---Income vs.Spending Scatter Plot ---")
plt.figure(figsize=(6, 4))
sns.scatterplot(x='Income', y='Spending', data=df, hue='Gender', alpha=0.7)
plt.title('Income vs. Spending Score')
plt.show()

"""## **Step 2: Applying K-Means Clustering**"""

# ---K-Means (K=5) ---
print("\nK-Means Clustering (K=5)")
kmeans = KMeans(n_clusters=K, init='k-means++', random_state=42, n_init='auto')
df['KMeans_Cluster'] = kmeans.fit_predict(X)
centroids = pd.DataFrame(kmeans.cluster_centers_, columns=['Income', 'Spending'])

plt.figure(figsize=(8, 5))
sns.scatterplot(x='Income', y='Spending', hue='KMeans_Cluster', data=df, palette='Set1', s=100)
plt.scatter(centroids['Income'], centroids['Spending'], marker='X', s=200, color='black')
plt.title(f'K-Means Clusters (K={K}) with Centroids')
plt.show()

"""###**Step 3: Determining the Optimal Number of Clusters**"""

# Optimal K Determination (Elbow & Silhouette) ---
print("\nOptimal K Determination Plots")
inertia = []
for k in range(1, 11):
    kmeans_temp = KMeans(n_clusters=k, init='k-means++', random_state=42, n_init='auto').fit(X)
    inertia.append(kmeans_temp.inertia_)

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(range(1, 11), inertia, marker='o', linestyle='--'); plt.title('Elbow Method'); plt.xlabel('K'); plt.ylabel('Inertia')

sil_scores = []
for k in range(2, 11):
    sil_scores.append(silhouette_score(X, KMeans(n_clusters=k, init='k-means++', random_state=42, n_init='auto').fit(X).labels_))

plt.subplot(1, 2, 2)
plt.plot(range(2, 11), sil_scores, marker='o', linestyle='--'); plt.title('Silhouette Score'); plt.xlabel('K'); plt.ylabel('Score')
plt.show()

"""### **Step 4: Cluster Profiling and Insights**

"""

#Cluster Profiling
print("\nCluster Profiling (Mean Values)")
profile = df.groupby('KMeans_Cluster')[['Income', 'Spending', 'Age']].mean().round(2)
profile['Segment'] = ['Average', 'High-Spender', 'Impulsive', 'Cautious', 'Frugal']
print(profile.sort_values('Income'))

"""### **Step 5: Applying Hierarchical Clustering**

"""

#Hierarchical Clustering
print("\nHierarchical Clustering Dendrogram & Plot")
linked_ward = linkage(X, method='ward')
plt.figure(figsize=(7, 6))
dendrogram(linked_ward, truncate_mode='lastp', p=10); plt.title("Dendrogram - Ward's Linkage"); plt.show()

hierarchical_model = AgglomerativeClustering(n_clusters=K, linkage='ward')
df['Hierarchical_Cluster'] = hierarchical_model.fit_predict(X)

plt.figure(figsize=(6, 4))
sns.scatterplot(x='Income', y='Spending', hue='Hierarchical_Cluster', data=df, palette='Set1', s=100)
plt.title(f'Hierarchical Clusters (Ward, K={K})')
plt.show()

"""### **Step 6: Comparing Hierarchical and K-Means Clustering**

"""

#Comparison & Evaluation ---
print("\nComparison & Evaluation Metrics")
comparison = pd.DataFrame({
    'Algorithm': ['K-Means', 'Hierarchical (Ward)'],
    'Silhouette Score': [silhouette_score(X, df['KMeans_Cluster']), silhouette_score(X, df['Hierarchical_Cluster'])],
    'Davies–Bouldin Index': [davies_bouldin_score(X, df['KMeans_Cluster']), davies_bouldin_score(X, df['Hierarchical_Cluster'])]
}).round(4)
print(comparison)

"""### **Step 7: Visualizing Clusters Using PCA**

"""

#PCA Visualization ---
print("\nPCA Visualization")
X_all = df[['Age', 'Income', 'Spending']].values
X_scaled = StandardScaler().fit_transform(X_all)
pca = PCA(n_components=2).fit(X_scaled)
pca_df = pd.DataFrame(data=pca.transform(X_scaled), columns=['PC1', 'PC2'])
pca_df['KMeans_Cluster'] = df['KMeans_Cluster']
pca_df['Hierarchical_Cluster'] = df['Hierarchical_Cluster']

plt.figure(figsize=(8, 4))
plt.subplot(1, 2, 1)
sns.scatterplot(x='PC1', y='PC2', hue='KMeans_Cluster', data=pca_df, palette='Set1', s=100); plt.title('K-Means in PCA Space')
plt.subplot(1, 2, 2)
sns.scatterplot(x='PC1', y='PC2', hue='Hierarchical_Cluster', data=pca_df, palette='Set1', s=100); plt.title('Hierarchical in PCA Space')
plt.show()

print(f"\nFinal Summary")
print("K-Means is marginally better (higher Silhouette, lower DBI) and provides highly interpretable 5 segments.")

"""# **CONCLUSION**
After digging into the data, the core finding is crystal clear: we can best segment our customers into **five distinct groups** using the $\text{K}$-Means method, which performed beautifully. This breakthrough immediately identifies our most valuable shoppers—the **High-Income High-Spenders**—allowing us to sharpen our marketing focus and drive better results.
"""